{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='t5-small'\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_type = 'resolution-order-labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEGODataset(Dataset):\n",
    "    def __init__(self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        source_max_token_length: int=100,\n",
    "        target_max_token_length: int=100,\n",
    "        ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_max_token_length = source_max_token_length\n",
    "        self.target_max_token_length = target_max_token_length\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, index: int):\n",
    "        row = self.data.iloc[index]\n",
    "        source_encoding = self.tokenizer(\n",
    "            row['input'],\n",
    "            max_length=self.source_max_token_length,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "        row[label_type],\n",
    "        max_length=self.target_max_token_length,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "        )\n",
    "        labels = target_encoding['input_ids']\n",
    "        labels[labels==tokenizer.pad_token_id]=-100\n",
    "        out = dict(\n",
    "            source=row['input'],\n",
    "            target=row[label_type],\n",
    "            input_ids=source_encoding['input_ids'].flatten(),\n",
    "            attention_mask=source_encoding['attention_mask'].flatten(),\n",
    "            labels=labels.flatten(),\n",
    "            len_source=len(source_encoding['input_ids'].flatten()),\n",
    "            len_target=len(labels.flatten())\n",
    "                   ) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEGODataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        source_max_token_length: int,\n",
    "        target_max_token_length: int,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        batch_size: int=16, \n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.source_max_token_length = source_max_token_length\n",
    "        self.target_max_token_length = target_max_token_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "  \n",
    "    def setup(self):\n",
    "        self.train_dataset = LEGODataset(\n",
    "            data=self.train_df, \n",
    "            tokenizer=self.tokenizer,\n",
    "            source_max_token_length=self.source_max_token_length,\n",
    "            target_max_token_length=self.target_max_token_length)\n",
    "        self.test_dataset = LEGODataset(\n",
    "            data=self.test_df, \n",
    "            tokenizer=self.tokenizer,\n",
    "            source_max_token_length=self.source_max_token_length,\n",
    "            target_max_token_length=self.target_max_token_length)\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                   batch_size=self.batch_size,\n",
    "                   shuffle=True, \n",
    "                   num_workers=2)\n",
    "  \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset,\n",
    "                   batch_size=1,\n",
    "                   shuffle=True, \n",
    "                   num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import ProgressBar\n",
    "class LEGOTask(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return output.loss, output.logits\n",
    "  \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"Training loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "  \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"Validation loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"Test loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss   \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experimenting(object):\n",
    "    def __init__(self, results_base_dir='../../lego-results', data_dict=None, source_max_token_length=100, target_max_token_length=100):\n",
    "        self.results_base_dir = results_base_dir\n",
    "        self.data_dict = data_dict\n",
    "        self.setup_name = self.data_dict['setup_name']\n",
    "        self.num_samples = len(self.data_dict['tr_paths'])\n",
    "        self.source_max_token_length = source_max_token_length\n",
    "        self.target_max_token_length = target_max_token_length\n",
    "        self.tr_te_path_pairs = []\n",
    "        self.train_dfs = []\n",
    "        self.test_dfs = []\n",
    "        self.checkpoints_path = None\n",
    "        self.trained_models_paths = []\n",
    "        \n",
    "        if not os.path.isdir(self.results_base_dir):\n",
    "            os.mkdir(self.results_base_dir)\n",
    "        self.checkpoints_path = os.path.join(self.results_base_dir, self.setup_name + '-checkpoints')\n",
    "        if os.path.isdir(self.checkpoints_path):\n",
    "            shutil.rmtree(self.checkpoints_path)\n",
    "            os.mkdir(self.checkpoints_path)\n",
    "        else:\n",
    "            os.mkdir(self.checkpoints_path)\n",
    "        \n",
    "        print(\"Experiment name: \", self.setup_name)\n",
    "        print(\"Starting to load datasets ...\")\n",
    "        for i in range(self.num_samples):\n",
    "            tr_path, te_path = self.data_dict['tr_paths'][i], self.data_dict['te_paths'][i]\n",
    "            self.tr_te_path_pairs.append((i, tr_path, te_path))\n",
    "            train_df = pd.read_csv(tr_path, encoding='utf-8', index_col=0)\n",
    "            #train_df['target_text'] = train_df['target_text'].map(str)\n",
    "            print(\"Training path: \", tr_path)\n",
    "            print(\"Training set size: \", len(train_df))\n",
    "            self.train_dfs.append(train_df)\n",
    "            test_df = pd.read_csv(te_path, encoding='utf-8', index_col=0)\n",
    "            #test_df['target_text'] = test_df['target_text'].map(str)\n",
    "            print(\"Test path: \", te_path)\n",
    "            print(\"Test set size: \", len(test_df))\n",
    "            self.test_dfs.append(test_df)\n",
    "            print(\"Done loading datasets for sample num {} ...\".format(i))\n",
    "        print(\"Done loading all datasets!\")    \n",
    "            \n",
    "    def run_setup(self):\n",
    "        for i in range(self.num_samples):\n",
    "            train_df, test_df = self.train_dfs[i], self.test_dfs[i]\n",
    "        \n",
    "            data_module = LEGODataModule(train_df=train_df, test_df=test_df, tokenizer=tokenizer, source_max_token_length=self.source_max_token_length, target_max_token_length=self.target_max_token_length, batch_size=BATCH_SIZE)\n",
    "            data_module.setup()\n",
    "            \n",
    "            print(\"A sample training data: \", data_module.train_dataset[0])\n",
    "            print(\"A sample test data: \", data_module.test_dataset[0])\n",
    "            print(\"A sample train data tokenized and decoded back: \")\n",
    "            print(\"---Input size: \", len(data_module.train_dataset[0]['input_ids']))\n",
    "            for j, t0 in enumerate(data_module.train_dataset[0]['input_ids']) :\n",
    "                print(\"index: {}-token-id: {} -->token: {}\".format(j+1, t0.item(), tokenizer.decode(t0, clean_up_tokenization_spaces=True))) \n",
    "            print(\"------------\")\n",
    "            print(\"---Label size: \", len(data_module.train_dataset[0]['labels']))\n",
    "            for k, t1 in enumerate(data_module.train_dataset[0]['labels']) :\n",
    "                print(\"index: {}-token-id: {} --> token: {}\".format(k+1, t1.item(), tokenizer.decode(t1, clean_up_tokenization_spaces=True))) \n",
    "            \n",
    "            final_model = LEGOTask()\n",
    "            checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=self.checkpoints_path,\n",
    "            filename='best-checkpoint-sample-num-{}'.format(i),\n",
    "            save_top_k=1,\n",
    "            verbose=True,\n",
    "            monitor='Validation loss',\n",
    "            mode='min'\n",
    "            )\n",
    "            train_logs_path = os.path.join(self.checkpoints_path, \"training-logs-samp-num-{}\".format(i))\n",
    "            logger = TensorBoardLogger(train_logs_path, name=\"Current task\")\n",
    "            trainer = pl.Trainer(\n",
    "            logger=logger,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            max_epochs=EPOCHS,\n",
    "            gpus=1,\n",
    "            progress_bar_refresh_rate=30\n",
    "            )\n",
    "            trainer.fit(final_model, data_module)\n",
    "            self.trained_models_paths.append((i, os.path.join(self.checkpoints_path, 'best-checkpoint-sample-num-{}.ckpt'.format(i))))\n",
    "        return self.trained_models_paths\n",
    "\n",
    "    def get_correctness_degree(self, x, y):\n",
    "        if x==y:\n",
    "            return 'correct', 1.\n",
    "        else:\n",
    "            if len(x) < len(y):\n",
    "                smaller = x\n",
    "                larger = y\n",
    "            else:\n",
    "                smaller = y\n",
    "                larger = x\n",
    "            overlap = 0.\n",
    "            for i in range(len(smaller)):\n",
    "                if smaller[i]==larger[i]:\n",
    "                    overlap+=1.\n",
    "            if overlap==0.:\n",
    "                return 'wrong', 0.\n",
    "            else:\n",
    "                return 'partial', overlap / len(larger) \n",
    "            \n",
    "    def evaluate(self):\n",
    "        setup_evaluation_results = []\n",
    "        for i, trained_model_path in self.trained_models_paths: \n",
    "            test_df = self.test_dfs[0]\n",
    "\n",
    "            print(\"Loading from best checkpoint for sample {} stored at {} \".format(i,  trained_model_path))\n",
    "            trained_model = LEGOTask.load_from_checkpoint(trained_model_path)\n",
    "            trained_model.freeze()\n",
    "            \n",
    "            val_dataset = LEGODataset(data=test_df, tokenizer=tokenizer)\n",
    "            val_recs, val_targets = [], []\n",
    "            for j in range(len(test_df)):\n",
    "                val_recs.append(val_dataset[j]['source'])\n",
    "                val_targets.append(val_dataset[j]['target'])\n",
    "            print(\"Size of the val recs: \", len(val_recs))     \n",
    "            val_recs = val_recs[:500]\n",
    "            val_targets = val_targets[:500]\n",
    "            val_encoding = tokenizer(val_recs, max_length=self.source_max_token_length, padding='max_length', return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\")\n",
    "            val_generated_ids = trained_model.model.generate(input_ids=val_encoding['input_ids'], attention_mask=val_encoding['attention_mask'], max_length=self.target_max_token_length)\n",
    "            preds = [tokenizer.decode(gen_id, clean_up_tokenization_spaces=True, skip_special_tokens=True) for gen_id in val_generated_ids]\n",
    "            #print(preds)\n",
    "\n",
    "            correct, partially_correct, totally_wrong = 0, 0, 0\n",
    "            correct_cases, partially_correct_cases, totally_wrong_cases = [], [], [] \n",
    "            for r in range(len(preds)):\n",
    "                case = {'indx' : r, 'source' : val_dataset[r]['source'], 'target': val_dataset[r]['target'], 'pred': preds[r]}\n",
    "                judgment, overlap_frac = self.get_correctness_degree(preds[r], val_targets[r])\n",
    "                if judgment=='correct':\n",
    "                    correct += 1.\n",
    "                    correct_cases.append(case)\n",
    "                elif judgment=='wrong':\n",
    "                    totally_wrong += 1.\n",
    "                    totally_wrong_cases.append(case)\n",
    "                else:\n",
    "                    partially_correct += 1.\n",
    "                    partially_correct_cases.append(case)\n",
    "        \n",
    "            setup_evaluation_results.append({'setup_name' : self.setup_name, 'sample_num': i, \"Accuracy\" : round(float(correct) / len(preds) * 100., 1), \"correct_cases\" : correct_cases, \"partially_correct_percentage\" : round(float(partially_correct) / len(preds) * 100., 1), \"partially_correct_cases\" : partially_correct_cases, \"totally_wrong_percentage\" : round(float(totally_wrong) / len(preds) * 100., 1), \"totally_wrong_cases\" : totally_wrong_cases})        \n",
    "            print(\"Accuracy for sample {}: {}%\".format(i, round(float(correct) / len(preds)*100., 1)))    \n",
    "            print(\"Finished evaluating setup {} for sample {}.\".format(self.setup_name, i))\n",
    "            print(\"---------------------------------------------------------------\")\n",
    "        print(\"Writing the results of evaluating the setup to json ...\")\n",
    "        output_file = open(os.path.join(self.results_base_dir, 'eval-results-for-setup-{}.json'.format(self.setup_name)), 'w', encoding='utf-8')\n",
    "        for dic in setup_evaluation_results:\n",
    "            json.dump(dic, output_file) \n",
    "            output_file.write('\\n')\n",
    "        print(\"Done!\")     \n",
    "        return setup_evaluation_results\n",
    "    \n",
    "    def infer(self, input_pair, path_to_model_checkpoint):\n",
    "        input_df = pd.DataFrame([input_pair], columns=['input', label_type])\n",
    "        trained_model = LEGOTask.load_from_checkpoint(path_to_model_checkpoint)\n",
    "        trained_model.freeze()\n",
    "        val_set = LEGODataset(data=input_df, tokenizer=tokenizer)\n",
    "        val_rec = val_set[0]['source']\n",
    "        val_encoding = tokenizer(val_rec, max_length=self.source_max_token_length, padding='max_length', return_attention_mask=True, add_special_tokens=True,return_tensors=\"pt\")\n",
    "        val_generated_ids = trained_model.model.generate(input_ids=val_encoding['input_ids'], attention_mask=val_encoding['attention_mask'], max_length=self.target_max_token_length)\n",
    "        preds = [tokenizer.decode(gen_id, clean_up_tokenization_spaces=True, skip_special_tokens=True) for gen_id in val_generated_ids]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'tr_paths': ['../../lego-data/lego-train.txt'],\n",
    "             'te_paths': ['../../lego-data/lego-test.txt'],\n",
    "             'setup_name': 'lego-default-setting'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name:  lego-default-setting\n",
      "Starting to load datasets ...\n",
      "Training path:  ../../lego-data/lego-train.txt\n",
      "Training set size:  6800\n",
      "Test path:  ../../lego-data/lego-test.txt\n",
      "Test set size:  1700\n",
      "Done loading datasets for sample num 0 ...\n",
      "Done loading all datasets!\n"
     ]
    }
   ],
   "source": [
    "exp = Experimenting(results_base_dir='../../lego-results', data_dict=data_dict, source_max_token_length=35, target_max_token_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample training data:  {'source': 't=-q; r=+t; a=+p; q=+s; p=-r; s=1', 'target': 's=1;q=1;t=-1;r=-1;p=1;a=1', 'input_ids': tensor([   3,   17, 2423,   18, 1824,  117,    3,   52, 2423, 1220,   17,  117,\n",
      "           3,    9, 2423, 1220,  102,  117,    3, 1824, 2423, 1220,    7,  117,\n",
      "           3,  102, 2423,   18,   52,  117,    3,    7, 2423,  536,    1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([   3,    7, 2423,  536,  117, 1824, 2423,  536,  117,   17, 2423, 2292,\n",
      "         117,   52, 2423, 2292,  117,  102, 2423,  536,  117,    9, 2423,  536,\n",
      "           1]), 'len_source': 35, 'len_target': 25}\n",
      "A sample test data:  {'source': 'n=-1; o=-n; t=+o; e=-i; i=-m; m=+t', 'target': 'n=-1;o=1;t=1;m=1;i=-1;e=1', 'input_ids': tensor([   3,   29, 2423, 2292,  117,    3,   32, 2423,   18,   29,  117,    3,\n",
      "          17, 2423, 1220,   32,  117,    3,   15, 2423,   18,   23,  117,    3,\n",
      "          23, 2423,   18,   51,  117,    3,   51, 2423, 1220,   17,    1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([   3,   29, 2423, 2292,  117,   32, 2423,  536,  117,   17, 2423,  536,\n",
      "         117,   51, 2423,  536,  117,   23, 2423, 2292,  117,   15, 2423,  536,\n",
      "           1]), 'len_source': 35, 'len_target': 25}\n",
      "A sample train data tokenized and decoded back: \n",
      "---Input size:  35\n",
      "index: 1-token-id: 3 -->token: \n",
      "index: 2-token-id: 17 -->token: t\n",
      "index: 3-token-id: 2423 -->token: =\n",
      "index: 4-token-id: 18 -->token: -\n",
      "index: 5-token-id: 1824 -->token: q\n",
      "index: 6-token-id: 117 -->token: ;\n",
      "index: 7-token-id: 3 -->token: \n",
      "index: 8-token-id: 52 -->token: r\n",
      "index: 9-token-id: 2423 -->token: =\n",
      "index: 10-token-id: 1220 -->token: +\n",
      "index: 11-token-id: 17 -->token: t\n",
      "index: 12-token-id: 117 -->token: ;\n",
      "index: 13-token-id: 3 -->token: \n",
      "index: 14-token-id: 9 -->token: a\n",
      "index: 15-token-id: 2423 -->token: =\n",
      "index: 16-token-id: 1220 -->token: +\n",
      "index: 17-token-id: 102 -->token: p\n",
      "index: 18-token-id: 117 -->token: ;\n",
      "index: 19-token-id: 3 -->token: \n",
      "index: 20-token-id: 1824 -->token: q\n",
      "index: 21-token-id: 2423 -->token: =\n",
      "index: 22-token-id: 1220 -->token: +\n",
      "index: 23-token-id: 7 -->token: s\n",
      "index: 24-token-id: 117 -->token: ;\n",
      "index: 25-token-id: 3 -->token: \n",
      "index: 26-token-id: 102 -->token: p\n",
      "index: 27-token-id: 2423 -->token: =\n",
      "index: 28-token-id: 18 -->token: -\n",
      "index: 29-token-id: 52 -->token: r\n",
      "index: 30-token-id: 117 -->token: ;\n",
      "index: 31-token-id: 3 -->token: \n",
      "index: 32-token-id: 7 -->token: s\n",
      "index: 33-token-id: 2423 -->token: =\n",
      "index: 34-token-id: 536 -->token: 1\n",
      "index: 35-token-id: 1 -->token: </s>\n",
      "------------\n",
      "---Label size:  25\n",
      "index: 1-token-id: 3 --> token: \n",
      "index: 2-token-id: 7 --> token: s\n",
      "index: 3-token-id: 2423 --> token: =\n",
      "index: 4-token-id: 536 --> token: 1\n",
      "index: 5-token-id: 117 --> token: ;\n",
      "index: 6-token-id: 1824 --> token: q\n",
      "index: 7-token-id: 2423 --> token: =\n",
      "index: 8-token-id: 536 --> token: 1\n",
      "index: 9-token-id: 117 --> token: ;\n",
      "index: 10-token-id: 17 --> token: t\n",
      "index: 11-token-id: 2423 --> token: =\n",
      "index: 12-token-id: 2292 --> token: -1\n",
      "index: 13-token-id: 117 --> token: ;\n",
      "index: 14-token-id: 52 --> token: r\n",
      "index: 15-token-id: 2423 --> token: =\n",
      "index: 16-token-id: 2292 --> token: -1\n",
      "index: 17-token-id: 117 --> token: ;\n",
      "index: 18-token-id: 102 --> token: p\n",
      "index: 19-token-id: 2423 --> token: =\n",
      "index: 20-token-id: 536 --> token: 1\n",
      "index: 21-token-id: 117 --> token: ;\n",
      "index: 22-token-id: 9 --> token: a\n",
      "index: 23-token-id: 2423 --> token: =\n",
      "index: 24-token-id: 536 --> token: 1\n",
      "index: 25-token-id: 1 --> token: </s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=30)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/miniconda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Missing logger folder: ../../lego-results/lego-default-setting-checkpoints/training-logs-samp-num-0/Current task\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:658: UserWarning: Your `val_dataloader` has `shuffle=True`, it is strongly recommended that you turn this off for val/test/predict dataloaders.\n",
      "  category=UserWarning,\n",
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 34. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
      "Global seed set to 123\n",
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa87069bc494102bd3eaae905006d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 33. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
      "Epoch 0, global step 849: Validation loss reached 0.00011 (best 0.00011), saving model to \"/mnt/task_runtime/lego-results/lego-default-setting-checkpoints/best-checkpoint-sample-num-0.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '../../lego-results/lego-default-setting-checkpoints/best-checkpoint-sample-num-0.ckpt')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.run_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from best checkpoint for sample 0 stored at ../../lego-results/lego-default-setting-checkpoints/best-checkpoint-sample-num-0.ckpt \n",
      "Size of the val recs:  1700\n",
      "Accuracy for sample 0: 100.0%\n",
      "Finished evaluating setup lego-default-setting for sample 0.\n",
      "---------------------------------------------------------------\n",
      "Writing the results of evaluating the setup to json ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sss = exp.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat ../../lego-results/eval-results-for-setup-lego-default-setting.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Sample number: 0**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Accuracy: 100.0%\n",
      "-Partially correct percentage: 0.0%\n",
      "-Totally wrong percentage: 0.0%\n",
      "\n",
      "-Correct cases:\n",
      "1.\n",
      "---Source:     n=-1; o=-n; t=+o; e=-i; i=-m; m=+t\n",
      "---Target:     n=-1;o=1;t=1;m=1;i=-1;e=1\n",
      "---Prediction: n=-1;o=1;t=1;m=1;i=-1;e=1\n",
      "2.\n",
      "---Source:     l=-j; c=+y; q=+c; y=-1; j=+q; d=-l\n",
      "---Target:     y=-1;c=-1;q=-1;j=-1;l=1;d=-1\n",
      "---Prediction: y=-1;c=-1;q=-1;j=-1;l=1;d=-1\n",
      "3.\n",
      "---Source:     m=-z; p=+h; z=-1; e=-g; h=+e; g=+m\n",
      "---Target:     z=-1;m=1;g=1;e=-1;h=-1;p=-1\n",
      "---Prediction: z=-1;m=1;g=1;e=-1;h=-1;p=-1\n",
      "4.\n",
      "---Source:     s=+r; r=-g; h=+n; l=-h; g=-l; n=-1\n",
      "---Target:     n=-1;h=-1;l=1;g=-1;r=1;s=1\n",
      "---Prediction: n=-1;h=-1;l=1;g=-1;r=1;s=1\n",
      "5.\n",
      "---Source:     a=+k; b=-a; s=+l; k=+s; p=+b; l=-1\n",
      "---Target:     l=-1;s=-1;k=-1;a=-1;b=1;p=1\n",
      "---Prediction: l=-1;s=-1;k=-1;a=-1;b=1;p=1\n",
      "6.\n",
      "---Source:     f=-d; d=1; g=-f; w=-s; z=+w; s=+g\n",
      "---Target:     d=1;f=-1;g=1;s=1;w=-1;z=-1\n",
      "---Prediction: d=1;f=-1;g=1;s=1;w=-1;z=-1\n",
      "7.\n",
      "---Source:     e=+f; f=-a; d=-n; a=-1; z=+d; n=+e\n",
      "---Target:     a=-1;f=1;e=1;n=1;d=-1;z=-1\n",
      "---Prediction: a=-1;f=1;e=1;n=1;d=-1;z=-1\n",
      "8.\n",
      "---Source:     m=+h; k=-c; b=-k; h=1; u=+m; c=+u\n",
      "---Target:     h=1;m=1;u=1;c=1;k=-1;b=1\n",
      "---Prediction: h=1;m=1;u=1;c=1;k=-1;b=1\n",
      "9.\n",
      "---Source:     a=-e; e=+x; m=+k; k=1; o=+m; x=+o\n",
      "---Target:     k=1;m=1;o=1;x=1;e=1;a=-1\n",
      "---Prediction: k=1;m=1;o=1;x=1;e=1;a=-1\n",
      "10.\n",
      "---Source:     q=+p; t=-1; s=+q; f=+r; r=+s; p=+t\n",
      "---Target:     t=-1;p=-1;q=-1;s=-1;r=-1;f=-1\n",
      "---Prediction: t=-1;p=-1;q=-1;s=-1;r=-1;f=-1\n",
      "11.\n",
      "---Source:     e=-y; r=+q; v=-1; y=+r; s=-e; q=+v\n",
      "---Target:     v=-1;q=-1;r=-1;y=-1;e=1;s=-1\n",
      "---Prediction: v=-1;q=-1;r=-1;y=-1;e=1;s=-1\n",
      "12.\n",
      "---Source:     c=+i; o=+x; q=-1; i=+o; r=-q; x=+r\n",
      "---Target:     q=-1;r=1;x=1;o=1;i=1;c=1\n",
      "---Prediction: q=-1;r=1;x=1;o=1;i=1;c=1\n",
      "13.\n",
      "---Source:     w=+h; j=-k; s=+u; u=-j; k=-w; h=1\n",
      "---Target:     h=1;w=1;k=-1;j=1;u=-1;s=-1\n",
      "---Prediction: h=1;w=1;k=-1;j=1;u=-1;s=-1\n",
      "14.\n",
      "---Source:     u=1; k=-m; y=-o; o=+k; a=+u; m=-a\n",
      "---Target:     u=1;a=1;m=-1;k=1;o=1;y=-1\n",
      "---Prediction: u=1;a=1;m=-1;k=1;o=1;y=-1\n",
      "15.\n",
      "---Source:     d=-x; f=-1; x=-i; i=+s; z=+f; s=+z\n",
      "---Target:     f=-1;z=-1;s=-1;i=-1;x=1;d=-1\n",
      "---Prediction: f=-1;z=-1;s=-1;i=-1;x=1;d=-1\n",
      "16.\n",
      "---Source:     a=1; x=-k; h=+r; k=-a; e=+x; r=+e\n",
      "---Target:     a=1;k=-1;x=1;e=1;r=1;h=1\n",
      "---Prediction: a=1;k=-1;x=1;e=1;r=1;h=1\n",
      "17.\n",
      "---Source:     n=1; a=+z; c=+p; m=+a; p=-n; z=+c\n",
      "---Target:     n=1;p=-1;c=-1;z=-1;a=-1;m=-1\n",
      "---Prediction: n=1;p=-1;c=-1;z=-1;a=-1;m=-1\n",
      "18.\n",
      "---Source:     g=-i; f=+u; w=1; v=-w; u=-v; i=+f\n",
      "---Target:     w=1;v=-1;u=1;f=1;i=1;g=-1\n",
      "---Prediction: w=1;v=-1;u=1;f=1;i=1;g=-1\n",
      "19.\n",
      "---Source:     c=-h; u=-n; n=+i; x=-u; i=-1; h=+x\n",
      "---Target:     i=-1;n=-1;u=1;x=-1;h=-1;c=1\n",
      "---Prediction: i=-1;n=-1;u=1;x=-1;h=-1;c=1\n",
      "20.\n",
      "---Source:     v=-d; c=+u; p=-c; h=-1; u=-h; d=-p\n",
      "---Target:     h=-1;u=1;c=1;p=-1;d=1;v=-1\n",
      "---Prediction: h=-1;u=1;c=1;p=-1;d=1;v=-1\n",
      "\n",
      "-Totally wrong cases:\n",
      "NONE\n",
      "\n",
      "-Partially correct cases:\n",
      "NONE\n",
      "\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "num_cases = 20\n",
    "def pprint_entry(entries, name):\n",
    "    print(\"-\"+name+\":\")\n",
    "    if not entries:\n",
    "        print(\"NONE\")\n",
    "    for i, entry in enumerate(entries):\n",
    "        print(\"{}.\".format(i+1)) \n",
    "        print('---Source:     {}'.format(entry['source']))\n",
    "        print('---Target:     {}'.format(entry['target']))\n",
    "        print('---Prediction: {}'.format(entry['pred'])) \n",
    "    print()\n",
    "              \n",
    "              \n",
    "with open('../../lego-results/eval-results-for-setup-lego-default-setting.json', 'r', encoding='utf-8') as input_file:\n",
    "    for line in input_file:\n",
    "        dic=json.loads(line)\n",
    "        printbold(\"Sample number: {}\".format(dic['sample_num']))\n",
    "        print(\"-Accuracy: {}%\".format(dic['Accuracy']))\n",
    "        print(\"-Partially correct percentage: {}%\".format(dic['partially_correct_percentage']))\n",
    "        print(\"-Totally wrong percentage: {}%\".format(dic['totally_wrong_percentage']))\n",
    "        print()\n",
    "        pprint_entry(dic['correct_cases'][:num_cases], 'Correct cases')\n",
    "        pprint_entry(dic['totally_wrong_cases'][:num_cases], 'Totally wrong cases')\n",
    "        pprint_entry(dic['partially_correct_cases'][:num_cases], \"Partially correct cases\")\n",
    "        print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h=-1;k=1;q=-1;e=1;r=1;i=1']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.infer(('e=-q; r=+e; q=-k; i=+r; k=-h; h=-1', 'h=-1;k=1;q=-1;e=1;r=1;i=1'), '/mnt/task_runtime/lego-results/lego-default-setting-checkpoints/best-checkpoint-sample-num-0.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e=1;g=1;m=1;d=-1;j=1;c=-1']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.infer(('g=+e; j=-d; d=-m; m=+g; a=-t; t=-o; w=+n; c=-j; o=-q; n=-a; q=+c; e=1;', 'e=1;g=1;m=1;d=-1;j=1;c=-1;q=-1;o=1;t=-1;a=1;n=-1;w=-1'), '/mnt/task_runtime/lego-results/lego-default-setting-checkpoints/best-checkpoint-sample-num-0.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
